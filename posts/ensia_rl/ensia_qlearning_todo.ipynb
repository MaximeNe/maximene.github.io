{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "We have just seen what Q-learning is a few minutes ago:\n",
    "It is one of the fundamental principles of RL. For those who want to go deeper, you can look for Temporal Difference Learning (TD learning).\n",
    "As a reminder, we can define Q-Learning as a technique for learning the Q-value, i.e. the quality of an action performed in a given state. Q-learning allows us to learn a policy (the choice that our agent must take).\n",
    "We also saw how to iterate to find the correct Q-values:  \n",
    "$$Q_{t}(S_{t-1}, a_{t-1}) = Q_{t-1}(S_{t-1}, a_{t-1}) + \\alpha_t (r_t + \\gamma \\max_{a'}(S_t, a') - Q_{t-1}(S_{t-1}, a_{t-1}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are going to use Q-learning to make our agent perform better that a random action in a defined environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In Reinforcement Learning, we always need an environment. We can either create one, or use one. The easiest way is to use some libraries that offers pre-defined environments but also a standardized way of building our own environments.  \n",
    "We will use [OpenAI Gym](https://gym.openai.com/), which offers a lot of turnkey environments, with complete documentations. We invite you to visit their website and discover what they offer. We will use OpenAI Gym again later! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use Python 3.9 for this lab work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's discover OpenAI Gym together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build environment for CartPole-v1\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Initialize\n",
    "env.reset()\n",
    "\n",
    "# And we can already use it !\n",
    "for _ in range(1000):\n",
    "  env.render()\n",
    "  action = env.action_space.sample() # Choose a random action\n",
    "  observation, reward, done, info = env.step(action)  # Perform the action\n",
    "  # In exchange, we receive the observations, the rewards, a boolean which indicates if the \"game\" is finished or not, and debugging information\n",
    "\n",
    "  if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In case of problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you have an error message when running the code above, we are sorry :(  \n",
    "Actually, the render doesn't work on google colab, and it's also possible that it doesn't work on some machines even if you use jupyter notebook...\n",
    "If this is your case, you can watch this video which shows what you should have seen. Afterwards, remove all the env.render(), the code will work, but you just won't get the visual rendering.\n",
    "\n",
    "http://s3-us-west-2.amazonaws.com/rl-gym-doc/cartpole-no-reset.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Taxi-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are going to use Taxi-v3 for its simplicity (the output is displayed in the terminal, so EVERYONE can do it, even on google colab), and also because the simpler the \"game\" is, the \"easier\" it will be to make our agent learn quickly, which is not bad when we have short TP...\n",
    "Anyway ...\n",
    "Now it's your turn to play ! Code something close to the previous example to create and visualize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# TODO\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Take a look at the doc to understand what the things you just visualized correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "help(gym.envs.toy_text.TaxiEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's create and train our agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# The hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.3\n",
    "train_episodes = 100000\n",
    "\n",
    "\n",
    "# Create an empty Q table:\n",
    "# TODO\n",
    "\n",
    "\n",
    "# A list to keep track of reward and epsilon values\n",
    "training_rewards = []\n",
    "\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    # TODO (perform actions and fill the Q table ; store the rewards for each episode in training_reward to be able to plot it later)\n",
    "    # Don't forget the Exploration vs Exploitation traide off!\n",
    "\n",
    "    ...\n",
    "\n",
    "    \n",
    "env.close()\n",
    "\n",
    "print(\"Training finished in {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the behaviour of the algo\n",
    "x = range(train_episodes)\n",
    "plt.plot(x, training_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.title('Total rewards per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_epochs = 0\n",
    "episodes = 100\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state]) # TODO : modifier Q en valeur à définir\n",
    "        state, _, done, _ = env.step(action)\n",
    "        epochs += 1\n",
    "\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(Q[observation,:])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Flappy bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The environment\n",
    "\n",
    "We will use a custom version of FlappyBird-v0 environment based on OpenAI Gym. The original version is available [here](https://github.com/Talendar/flappy-bird-gym). In the custom version, we added few more params to describe the states.\n",
    "So, first, let's see what the environment looks like !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the flappy bird environment using OpenAI Gym\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Initialize the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Loop until the game is over\n",
    "while True:\n",
    "    # Next action to take:\n",
    "    action = env.action_space.sample() # Chose a random action (flap or none)\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "    # Rendering the game:\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "\n",
    "    # Checking if the player is still alive\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happened there ?\n",
    "Well, since we are taking a lot of (bad) decisions, the agent is doing everything wrong.\n",
    "Let's make a real agent that uses Q-Learning to play the game, instead of taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flappybot:\n",
    "    \"\"\"\n",
    "    A flappy bird agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Generate and initialize the agent\n",
    "        \"\"\"\n",
    "        self.nb_of_iteration = 0\n",
    "\n",
    "        # alive = 0 => add 1 to the total reward\n",
    "        # dead = 1 => add -1000 to the total reward\n",
    "        self.rewards = {0: 1, 1: -1000}\n",
    "\n",
    "        # Q learning params:\n",
    "        self.gamma = 1\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # State of the agent\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "        # Last action taken\n",
    "        self.last_action = 0\n",
    "\n",
    "        # Load stored Q values if they exist\n",
    "        self.load_qvalues()\n",
    "\n",
    "        # History of the choices taken during the game\n",
    "        self.moves = []\n",
    "\n",
    "    def load_qvalues(self):\n",
    "        \"\"\"\n",
    "        Load q values from a JSON file\n",
    "        \"\"\"\n",
    "        self.qvalues = {\"0_0_0\": 0}  # a key is x_y_action\n",
    "        try:\n",
    "            fil = open(\"./data/qvalues.json\", \"r\")\n",
    "            self.qvalues = json.load(fil)\n",
    "        except IOError:\n",
    "            return\n",
    "        fil.close()\n",
    "        \n",
    "    def update_qvalues(self):\n",
    "        \"\"\"\n",
    "        Update Q values once the game is over\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO :\n",
    "        # For every move the agent made except the last 3, update the q value using the +1 reward (don't forget that states are encoded in the q table using the get_key() method)\n",
    "        # For the last 3 moves, update q values using -1000 reward\n",
    "        # (We consider that the last 3 moves were \"bad\" while the others were \"good\")\n",
    "        ...\n",
    "\n",
    "\n",
    "    def get_key(self, previous_state, action):\n",
    "        \"\"\"\n",
    "        Generate a key x_y_action\n",
    "        :param previous_state:\n",
    "        :param action:\n",
    "        :return: x_y_action: str\n",
    "        \"\"\"\n",
    "        return str(previous_state[0]) + '_' + str(previous_state[1]) + '_' + str(action)\n",
    "\n",
    "    def save_qvalues(self):\n",
    "        \"\"\"\n",
    "        Save the qvalues in the JSON file\n",
    "        \"\"\"\n",
    "        f = open(\"./data/qvalues.json\", \"w\")\n",
    "        json.dump(self.qvalues, f)\n",
    "        f.close()\n",
    "    \n",
    "    def take_action(self, obs):\n",
    "        \"\"\"\n",
    "        Backup last move in self.moves and returns the best action to take\n",
    "        :param obs: observation returned by env.step()\n",
    "        :return: 0 or 1 : action to take\n",
    "        \"\"\"\n",
    "        self.nb_of_iteration += 1\n",
    "\n",
    "        x = int(obs[0]*1000) # in the original version, coordinates are float and quite small, but q table must have discrete values\n",
    "        y = int(obs[1]*1000)\n",
    "\n",
    "        # save the last move taken\n",
    "        self.moves.append(((self.x, self.y), (x, y), self.last_action))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "        # TODO\n",
    "        # Find the best action to take and return it. Pay attention to states we've never been into.\n",
    "        # (you should also update the last action)\n",
    "\n",
    "        ...\n",
    "\n",
    "    def show_current_position(self):\n",
    "        \"\"\"\n",
    "        Used to print the agent position during the game\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"X = \" + str(self.x) + \" | Y = \" + str(self.y))\n",
    "\n",
    "    def print_nb_steps(self, epoch):\n",
    "        \"\"\"\n",
    "        Used to print the current number of step the agent survived\n",
    "        :param epoch: actual epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"epoch \" + str(epoch) + \" -> \" + str(self.nb_of_iteration) + \" steps\")\n",
    "\n",
    "    def reset_counter(self):\n",
    "        \"\"\"\n",
    "        Reset local variables between each epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.nb_of_iteration = 0\n",
    "        self.moves = []\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.last_action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to train the agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING = Fa lse\n",
    "EPOCH = 30000\n",
    "VERBOSE = True\n",
    "\n",
    "# Create the flappy bird environment using OpenAI Gym\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Generate the agent\n",
    "agent = Flappybot()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # Initialize the environment\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        # Next action to take:\n",
    "        action = agent.take_action(obs)\n",
    "\n",
    "        # Processing:\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "        if not TRAINING:\n",
    "            # Rendering the game:\n",
    "            env.render()\n",
    "            time.sleep(1 / 30)  # FPS\n",
    "\n",
    "        # Checking if the player is still alive\n",
    "        if done:\n",
    "            if VERBOSE:\n",
    "                agent.print_nb_steps(epoch)\n",
    "            if TRAINING:\n",
    "                agent.update_qvalues()\n",
    "            agent.reset_counter()\n",
    "            break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Save the Q values in the JSON file\n",
    "agent.save_qvalues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, to see our agent perform, just set TRAINING to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Well, we can use Double Q-Learning to improve our agents, but the best idea is to use Deep Q-Learning, and that's what we are going to see next year!\n",
    "https://en.wikipedia.org/wiki/Q-learning"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13e883d4b5c27dfdde7a3405e52a76e07702e9943416aff15bd64afbf9a07dcb"
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
